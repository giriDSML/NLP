{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Analytics -- Movie Reviews Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to detail the analysis of a movie review  from scratch.\n",
    "Initially we will using machine learning with NLTK and then we will move on to Word2Vec using the deep learning.\n",
    "\n",
    "\n",
    "Steps:\n",
    "\n",
    "Phase-1 \n",
    "--------\n",
    "\n",
    "1) We will load a review data set\n",
    "\n",
    "2) Pefrom the preprocessing of the text\n",
    "\n",
    "3) convert the text to numerical values, i.e. vectorize the text ( we will use the tf-idf model) \n",
    "\n",
    "4) Use the Naive Bayes multinomial model to predict the review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_local_path='E:/P/DataScience/ML/DL/NLP/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(my_local_path+'labeledTrainData.tsv',delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 3 columns):\n",
      "id           25000 non-null object\n",
      "sentiment    25000 non-null int64\n",
      "review       25000 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 586.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12500\n",
       "0    12500\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So ,there is no class imbalance in the data. Had it been there, we would need to get the thresholds to be adjusted using the \n",
    "ROC AUC Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of the Text\n",
    "\n",
    "1) Case Conversion - all the words to lower case\n",
    "\n",
    "2) Removal of the stop words\n",
    "\n",
    "3) Removal of the Punctuations\n",
    "\n",
    "4) Spell correction  ( using TextBlob, a wrapper on top of NLTK)\n",
    "\n",
    "5) Tokenization\n",
    "\n",
    "6) Stemming\n",
    "\n",
    "7) lemmatization\n",
    "\n",
    "8) POS Tagging\n",
    "\n",
    "9) Named Entity Recognition ( NER) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will define a function to clean up the message like removing punctuations, single letter words etc..\n",
    "def clean_str(string):\n",
    "  \"\"\"\n",
    "  String cleaning before vectorization\n",
    "  \"\"\"\n",
    "  try:    \n",
    "    string = re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n",
    "    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n",
    "    words = string.strip().lower().split()    \n",
    "    words = [w for w in words if len(w)>=1]\n",
    "    return \" \".join(words)\t\n",
    "  except:\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review']=df['review'].apply(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>with all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>the classic war of the worlds by timothy hines...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>the film starts with a manager nicholas bell g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>it must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy and wondrously unpretentious s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8196_8</td>\n",
       "      <td>1</td>\n",
       "      <td>I dont know why people think this is such a ba...</td>\n",
       "      <td>i dont know why people think this is such a ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7166_2</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie could have been very good, but come...</td>\n",
       "      <td>this movie could have been very good but comes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10633_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I watched this video at a friend's house. I'm ...</td>\n",
       "      <td>i watched this video at a friend s house i m g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>319_1</td>\n",
       "      <td>0</td>\n",
       "      <td>A friend of mine bought this film for £1, and ...</td>\n",
       "      <td>a friend of mine bought this film for and even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8713_10</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;This movie is full of references. ...</td>\n",
       "      <td>br br this movie is full of references like ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2486_3</td>\n",
       "      <td>0</td>\n",
       "      <td>What happens when an army of wetbacks, towelhe...</td>\n",
       "      <td>what happens when an army of wetbacks towelhea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6811_10</td>\n",
       "      <td>1</td>\n",
       "      <td>Although I generally do not like remakes belie...</td>\n",
       "      <td>although i generally do not like remakes belie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11744_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\Mr. Harvey Lights a Candle\\\" is anchored by a...</td>\n",
       "      <td>mr harvey lights a candle is anchored by a bri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>7369_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I had a feeling that after \\Submerged\\\", this ...</td>\n",
       "      <td>i had a feeling that after submerged this one ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12081_1</td>\n",
       "      <td>0</td>\n",
       "      <td>note to George Litman, and others: the Mystery...</td>\n",
       "      <td>note to george litman and others the mystery s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3561_4</td>\n",
       "      <td>0</td>\n",
       "      <td>Stephen King adaptation (scripted by King hims...</td>\n",
       "      <td>stephen king adaptation scripted by king himse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4489_1</td>\n",
       "      <td>0</td>\n",
       "      <td>`The Matrix' was an exciting summer blockbuste...</td>\n",
       "      <td>the matrix was an exciting summer blockbuster ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3951_2</td>\n",
       "      <td>0</td>\n",
       "      <td>Ulli Lommel's 1980 film 'The Boogey Man' is no...</td>\n",
       "      <td>ulli lommel s film the boogey man is no classi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3304_10</td>\n",
       "      <td>1</td>\n",
       "      <td>This movie is one among the very few Indian mo...</td>\n",
       "      <td>this movie is one among the very few indian mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>9352_10</td>\n",
       "      <td>1</td>\n",
       "      <td>Most people, especially young people, may not ...</td>\n",
       "      <td>most people especially young people may not un...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review  \\\n",
       "0    5814_8          1  With all this stuff going down at the moment w...   \n",
       "1    2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2    7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3    3630_4          0  It must be assumed that those who praised this...   \n",
       "4    9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "5    8196_8          1  I dont know why people think this is such a ba...   \n",
       "6    7166_2          0  This movie could have been very good, but come...   \n",
       "7   10633_1          0  I watched this video at a friend's house. I'm ...   \n",
       "8     319_1          0  A friend of mine bought this film for £1, and ...   \n",
       "9   8713_10          1  <br /><br />This movie is full of references. ...   \n",
       "10   2486_3          0  What happens when an army of wetbacks, towelhe...   \n",
       "11  6811_10          1  Although I generally do not like remakes belie...   \n",
       "12  11744_9          1  \\Mr. Harvey Lights a Candle\\\" is anchored by a...   \n",
       "13   7369_1          0  I had a feeling that after \\Submerged\\\", this ...   \n",
       "14  12081_1          0  note to George Litman, and others: the Mystery...   \n",
       "15   3561_4          0  Stephen King adaptation (scripted by King hims...   \n",
       "16   4489_1          0  `The Matrix' was an exciting summer blockbuste...   \n",
       "17   3951_2          0  Ulli Lommel's 1980 film 'The Boogey Man' is no...   \n",
       "18  3304_10          1  This movie is one among the very few Indian mo...   \n",
       "19  9352_10          1  Most people, especially young people, may not ...   \n",
       "\n",
       "                                         clean_review  \n",
       "0   with all this stuff going down at the moment w...  \n",
       "1   the classic war of the worlds by timothy hines...  \n",
       "2   the film starts with a manager nicholas bell g...  \n",
       "3   it must be assumed that those who praised this...  \n",
       "4   superbly trashy and wondrously unpretentious s...  \n",
       "5   i dont know why people think this is such a ba...  \n",
       "6   this movie could have been very good but comes...  \n",
       "7   i watched this video at a friend s house i m g...  \n",
       "8   a friend of mine bought this film for and even...  \n",
       "9   br br this movie is full of references like ma...  \n",
       "10  what happens when an army of wetbacks towelhea...  \n",
       "11  although i generally do not like remakes belie...  \n",
       "12  mr harvey lights a candle is anchored by a bri...  \n",
       "13  i had a feeling that after submerged this one ...  \n",
       "14  note to george litman and others the mystery s...  \n",
       "15  stephen king adaptation scripted by king himse...  \n",
       "16  the matrix was an exciting summer blockbuster ...  \n",
       "17  ulli lommel s film the boogey man is no classi...  \n",
       "18  this movie is one among the very few indian mo...  \n",
       "19  most people especially young people may not un...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spell correction. I dont recommend this unless you see the data set has lot of spell mistakes. This is very expensive piece of code takes lot of time to execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import  TextBlob\n",
    "df['clean_review']=df['clean_review'].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS tagging and Lemmatization\n",
    "\n",
    "Its advised to perform the POS(Part of Speech) Tagging before the lemmatization to get the acual 'lemma's \n",
    "\n",
    "The wordnet Lemmatizer does take into consideration of the POS tags\n",
    "\n",
    "For example\n",
    "\n",
    " WordNetLemmatizer().lemmatize(\"loving\")\n",
    "\n",
    "'loving'\n",
    "\n",
    "WordNetLemmatizer().lemmatize('loving','v')\n",
    "\n",
    "'love'\n",
    "\n",
    "So it considers everything as Noun if we dont pass the POS Tag.\n",
    "\n",
    "The wordnet lemmatizer only knows four parts of speech (ADJ, ADV, NOUN, and VERB). We need to convert the POS tags to format understood by wordnet. \n",
    "Here i am considering only these 4 Tags. you can try all the POS tags from Penn Treebank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def penn2wordnet(tag):\n",
    "    if tag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif tag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    elif tag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif tag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(inputText):\n",
    "    pos_tag=nltk.pos_tag(nltk.word_tokenize(inputText))\n",
    "    outputText=[]\n",
    "    for word,tag in pos_tag:\n",
    "        wntag=penn2wordnet(tag)\n",
    "        if wntag is None:\n",
    "            outputText.append(WordNetLemmatizer().lemmatize(word))\n",
    "#            print(outputText)\n",
    "        else:\n",
    "            outputText.append(WordNetLemmatizer().lemmatize(word,wntag))\n",
    "#            print(outputText)\n",
    "    return ' '.join(outputText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the function to the clean_review text of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_review']=df['clean_review'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "      <td>with all this stuff go down at the moment with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "      <td>the classic war of the world by timothy hines ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "      <td>the film start with a manager nicholas bell gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "      <td>it must be assume that those who praise this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "      <td>superbly trashy and wondrously unpretentious s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review  \\\n",
       "0  5814_8          1  With all this stuff going down at the moment w...   \n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...   \n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...   \n",
       "3  3630_4          0  It must be assumed that those who praised this...   \n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8...   \n",
       "\n",
       "                                        clean_review  \n",
       "0  with all this stuff go down at the moment with...  \n",
       "1  the classic war of the world by timothy hines ...  \n",
       "2  the film start with a manager nicholas bell gi...  \n",
       "3  it must be assume that those who praise this f...  \n",
       "4  superbly trashy and wondrously unpretentious s...  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the words have been convereted to their lemmas. You can try stemming as an alternate approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next activity could be identifying the Named Entity Recognitions NER.\n",
    "\n",
    "Its about information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc. \n",
    "\n",
    "For ex: United states : This represents a country and this text being in 2 parts does not help us in predicting the output accurately\n",
    "\n",
    "But for our use case , its not so relevant so we will not perform this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will conver the text message to numeric values( in a vector) that a machine understands. For this purpose we will use TF-IDF(Term freequencey /Invese document freequency) converter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the conversion , we will remove the stop words as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words=stopwords.words('English')\n",
    "vectorizer=TfidfVectorizer(use_idf=True,lowercase=True,strip_accents='ascii',stop_words=stop_words,max_features=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df=pd.DataFrame()\n",
    "clean_df['review']=df['clean_review']\n",
    "clean_df['sentiment']=df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=20000, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',... 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"],\n",
       "        strip_accents='ascii', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=vectorizer.fit_transform(clean_df.review)\n",
    "Y=clean_df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 53360)\t0.03382431451126539\n",
      "  (0, 22611)\t0.0494022613806681\n",
      "  (0, 36384)\t0.027846133141277738\n",
      "  (0, 36184)\t0.7826918259125908\n",
      "  (0, 52627)\t0.0490623918302241\n",
      "  (0, 32370)\t0.039550996218539344\n",
      "  (0, 37260)\t0.05515344421511069\n",
      "  (0, 60699)\t0.04916467595315593\n",
      "  (0, 39130)\t0.039800396501580765\n",
      "  (0, 15423)\t0.03751090556928087\n",
      "  (0, 61686)\t0.07278959679939737\n",
      "  (0, 36593)\t0.1352104958827194\n",
      "  (0, 34591)\t0.0864681267049137\n",
      "  (0, 60572)\t0.06349384858106918\n",
      "  (0, 22117)\t0.015053718843290174\n",
      "  (0, 8823)\t0.037275292754472396\n",
      "  (0, 27743)\t0.0461269362189857\n",
      "  (0, 23781)\t0.05102403500708591\n",
      "  (0, 55680)\t0.03505046071148595\n",
      "  (0, 45149)\t0.0356545877579731\n",
      "  (0, 11518)\t0.0712410378807852\n",
      "  (0, 16927)\t0.05260937228132301\n",
      "  (0, 33626)\t0.02707075086222816\n",
      "  (0, 35797)\t0.028444617564664845\n",
      "  (0, 61149)\t0.03632291335629353\n",
      "  :\t:\n",
      "  (0, 28320)\t0.051539941151255333\n",
      "  (0, 5178)\t0.02141957988014816\n",
      "  (0, 7274)\t0.04450992435476921\n",
      "  (0, 22360)\t0.026255284423782402\n",
      "  (0, 57196)\t0.030913789148438037\n",
      "  (0, 54667)\t0.039398909047794056\n",
      "  (0, 18284)\t0.021594096659262447\n",
      "  (0, 23004)\t0.04383361330378684\n",
      "  (0, 42181)\t0.04325737461541563\n",
      "  (0, 60956)\t0.03352976225493559\n",
      "  (0, 3337)\t0.0356205189403926\n",
      "  (0, 53487)\t0.036364344885853134\n",
      "  (0, 25478)\t0.05881118438451713\n",
      "  (0, 14574)\t0.02894298452086081\n",
      "  (0, 4825)\t0.03330868238683478\n",
      "  (0, 10131)\t0.06383410943512097\n",
      "  (0, 15672)\t0.04059793334385758\n",
      "  (0, 18902)\t0.025157776138376067\n",
      "  (0, 16956)\t0.0302009575822005\n",
      "  (0, 18790)\t0.03476639908073908\n",
      "  (0, 53399)\t0.031730193176613346\n",
      "  (0, 50342)\t0.04102800947684268\n",
      "  (0, 32020)\t0.060836550918880426\n",
      "  (0, 25830)\t0.031129453470267555\n",
      "  (0, 31410)\t0.04337922495437637\n"
     ]
    }
   ],
   "source": [
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'latter'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()[12616]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zwick'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,random_state=121)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18750, 25000)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18750,)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We will train the Naive Bayes model with the train data (Conditional probability model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=naive_bayes.MultinomialNB()\n",
    "model=nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(x_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2724,  460],\n",
       "       [ 361, 2705]], dtype=int64)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,f1_score,accuracy_score,classification_report\n",
    "conf=confusion_matrix(y_pred,y_test)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86864"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.86      0.87      3184\n",
      "          1       0.85      0.88      0.87      3066\n",
      "\n",
      "avg / total       0.87      0.87      0.87      6250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report=classification_report(y_pred,y_test)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got an accuracy score of 86.86% which is not great & not bad. Lets try with other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfmodel=RandomForestClassifier(criterion='entropy')\n",
    "rfmodel.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf=rfmodel.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77376"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred_rf,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # So far Naive Bayes is better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we will perform the vector conversion using Deep learning Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Keras to work with the embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=train_test_split(df['clean_review'],df['sentiment'],random_state=111,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000,)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=t.texts_to_sequences(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to pad sequences to make all the reviews uniform length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length=400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=sequence.pad_sequences(X_train,maxlen=max_review_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=sequence.pad_sequences(X_test,maxlen=max_review_length,padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   81,    5,    1,   16,   72,   84,  104,   46,   48,   14,\n",
       "          2,    1,  599,    5,   28,    1, 7087,    4, 2962,   12,   14,\n",
       "         96,  231,  189,   91,   63,   26,  102,   14,   27,    7,   39,\n",
       "         78, 2092, 3872,   24,   14,    1,    4, 1509,    6,    2,  820,\n",
       "          9,    1,  194,  422,   19,   86,    1, 1640,    6,  653,  124,\n",
       "          1,  246,    5, 1489,    4,  914,    4, 2069,   18,  150,  441,\n",
       "         91, 1636, 1304, 5165,   41,  162,   18,    1,  176,  959,    5,\n",
       "       6801,    6,   41,  274,   19,   86,    3,  208,   53,   17,   48,\n",
       "          2,   57,   22,  189,   41,  108,   90, 1445, 2844,   13,   52,\n",
       "         37,    1,   88,  112,   16,   96,  475,   87,   25,  196,  322,\n",
       "        207,    3,  162,    9,   12, 7230,   72,    2,   95,    3,  334,\n",
       "        180,   37,  587, 8840,   33, 2844,   24,    2,  110,   29,    1,\n",
       "        344,    5,    1, 1532, 2348,    4,  910,    6,   63,   26,   55,\n",
       "         31,    3, 1479,   18,  121,   79,  123,    6,  682,   59,  107,\n",
       "       1485,    6,   25, 5479,    5,    1, 1375,    3,   24,    2,  123,\n",
       "          6,   27,   25, 3016,    6,   25,  573,   21,   50,  119,  185,\n",
       "         48,   24,   84,    2, 2023,    5,   27,   64,    1,  117,  161,\n",
       "         27,    3,  403,  274,   33,    3,    1, 1988,   34,  168,    1,\n",
       "       7791,   29,    1, 1207,    5,  286, 1821, 6563,    3,    3,  126,\n",
       "         37, 2134,   13,  508,    4,  831, 1102,    6,   25, 3447,    4,\n",
       "       3930,    3,   26,    5,    1, 3039,   13,  751,    6,   96, 2134,\n",
       "         46,    2,    3,  334, 1365,    6,    1,  162,   12,   14,    2,\n",
       "       2669,   97,   32, 5962, 3907, 1577, 4206, 2376,    4, 1410, 6821,\n",
       "         24,  154,   20,  308,   79,   47,   29,   28,   19,   24,   14,\n",
       "          1,   77,    4, 6475,  182, 2772,    4, 2175,   84,   42,   53,\n",
       "        162,    6,   73,   48,   24,   50,   27,  961,    3,  334,  180,\n",
       "          9, 3529,    8,    8,    9,    1,   81,   26,   14,    6, 3588,\n",
       "        695, 9641,   17,  786,    6, 6053, 1173,    4,  282,   11,  198,\n",
       "          6,    3, 1007,   12,  764,   22,    1,  939,    5,    7, 3519,\n",
       "        252,   78, 1063,  103,   25, 1243,   29,    1,  423,   59,  322,\n",
       "        463,   77,    2, 4768,    4,   14,    1,  176, 1516,    3, 1085,\n",
       "        958,   19,  214,  185,    1, 3376,  447,  252,   78,  172,   33,\n",
       "        122,  345,   25,  144,    5,  322,  463, 2747,    1, 2195,    4,\n",
       "        224,   21, 2179,   87,    1,  194,  247,   13,   86,  501, 3400,\n",
       "       2036,  872,   34,  383,   28,  290, 2134,   16,   24,    2,  502,\n",
       "          6,  110,   18,  204,  139,    4,  110,   25,  381,   87,    3,\n",
       "        334, 7786,   18,  103])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[212]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  11,   15,    2,  132,   10,  104,   11, 3320,   10,  438,   20,\n",
       "        104,   11,   15, 1635,  331,   10,    2,   38, 1199,   72,  104,\n",
       "          7,  127,   72,   14,   36,    7,   22,  251,  157,    4,  116,\n",
       "       2413,    6,  104,    7,    4,  461,    4,   14,  469,  223,  903,\n",
       "          9,  461,    7,   26,   85, 2155,  425,   12,    7,    2,    2,\n",
       "        485,  151,   10,  213,   80,   12,    7,    2,   23,  267,   47,\n",
       "         21,    2,    3, 1266,  392,   21,   84,   99,   11,   15,    1,\n",
       "       1266,    2, 1117,   62, 8585,    4,    1,   15,    2,   62,   27,\n",
       "          7,    2,  446,   26,   72,   84,    2, 2196,   17,  273,  350,\n",
       "        275, 2906,   72,   84,    2,  319,    7,    6,  273,  223,    1,\n",
       "       1272,   31,  211,  126,    4, 1266,    9,   11,   16,    2,   38,\n",
       "       5952,    4,   31,   21,  116,    6,  506,   53,   49,   18,  129,\n",
       "       1266,   11,    2,  446,    3,   15,   12,   72,   84,   58,  469,\n",
       "         49,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see the above, padding has been done for the reviews less than 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "Embeding_Vector_Length=100 # This determines the number of elements per vector representation of each of the 5000 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Embedding,Dense,Dropout,Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to define the word embedding layer of Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Embedding(top_words+1, # size of the vocabulary\n",
    "                    Embeding_Vector_Length, # size of the vector\n",
    "                    input_length=max_review_length\n",
    "                   )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(200,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(100,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(60,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(40,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We need to execute the graph now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 116s 6ms/sample - loss: 0.5055 - acc: 0.7223 - val_loss: 0.3200 - val_acc: 0.8692\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 167s 8ms/sample - loss: 0.1406 - acc: 0.9480 - val_loss: 0.3672 - val_acc: 0.8592\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 164s 8ms/sample - loss: 0.0154 - acc: 0.9958 - val_loss: 0.6904 - val_acc: 0.8494\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 189s 9ms/sample - loss: 0.0015 - acc: 0.9997 - val_loss: 0.8407 - val_acc: 0.8546\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 209s 10ms/sample - loss: 1.0252e-04 - acc: 1.0000 - val_loss: 0.9138 - val_acc: 0.8554\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 186s 9ms/sample - loss: 1.8151e-05 - acc: 1.0000 - val_loss: 0.9668 - val_acc: 0.8558\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 205s 10ms/sample - loss: 9.5617e-06 - acc: 1.0000 - val_loss: 1.0096 - val_acc: 0.8550\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 205s 10ms/sample - loss: 5.5717e-06 - acc: 1.0000 - val_loss: 1.0471 - val_acc: 0.8548\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 181s 9ms/sample - loss: 3.5160e-06 - acc: 1.0000 - val_loss: 1.0802 - val_acc: 0.8548\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 194s 10ms/sample - loss: 2.3608e-06 - acc: 1.0000 - val_loss: 1.1089 - val_acc: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x3a5b0d30>"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,epochs=10,batch_size=128,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, After using the Word2Vec , deep learnign as well, our accuracy score is more or less the same value(86.9%) . I hope this will give you an idea of how do we analyse the text using both the approaches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
