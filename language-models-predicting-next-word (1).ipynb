{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# source text format . For example if below is the input sequence, we will feed the data to the model as given in the \n#next line. We are going to use davincicode.txt file as the input which has more than 100K sentenses\ndata = 'Jack and Jill went up the hill To fetch a pail of water Jack fell down and broke his crown'","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## We will feed the model, Line by Line sequence of text and genrate the output "},{"metadata":{},"cell_type":"markdown","source":"            X,\t\t\t\t\t\ty\n\n        _, _, _, _, _, Jack, \t\t\t\tand\n\n\n        _, _, _, _, Jack, and \t\t\t\tJill\n\n        _, _, _, Jack, and, Jill,\t\t\twent\n\n        _, _, Jack, and, Jill, went,\t\tup\n\n        _, Jack, and, Jill, went, up,\t\tthe\n\n        Jack, and, Jill, went, up, the,\t\thill"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport re","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"book_text=open('../input/davincicodetxt/davincicode.txt',encoding='UTF-8').read()","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(book_text)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"842670"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences=list()\nfor line in book_text.split('.'):\n    sequences.append(line)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(sequences)","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"12293"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequences[1:10]","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"['',\n '',\n ' AGAIN',\n ' MORE THAN EVER',\n ' \\n\\n\\n\\nAcknowledgments \\n\\nFirst and foremost, to my friend and editor, Jason Kaufman, for working so hard on this project and \\nfor truly understanding what this book is all about',\n ' And to the incomparable Heide Lange â€” tireless \\nchampion of The Da Vinci Code, agent extraordinaire, and trusted friend',\n ' \\n\\nI cannot fully express my gratitude to the exceptional team at Doubleday, for their generosity, faith, \\nand superb guidance',\n ' Thank you especially to Bill Thomas and Steve Rubin, who believed in this \\nbook from the start',\n \" My thanks also to the initial core of early in-house supporters, headed by \\nMichael Palgon, Suzanne Herz, Janelle Moburg, Jackie Everly, and Adrienne Sparks, as well as to \\nthe talented people of Doubleday's sales force\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_str(string):\n  \"\"\"\n  String cleaning before vectorization\n  \"\"\"\n  try:    \n    string = re.sub(r'^https?:\\/\\/<>.*[\\r\\n]*', '', string, flags=re.MULTILINE)\n    string = re.sub(r\"[^A-Za-z]\", \" \", string)         \n    words = string.strip().lower().split()    \n    words = [w for w in words if len(w)>=1]\n    if len(words)>1:\n        return \" \".join(words)\t\n    else:\n        return 'NA'\n    \n  except:\n    return \"\"","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_seq=list() ","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in sequences:\n    cleaned_seq.append(clean_str(line))\n    ","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_seq[1:10]","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"['NA',\n 'NA',\n 'NA',\n 'more than ever',\n 'acknowledgments first and foremost to my friend and editor jason kaufman for working so hard on this project and for truly understanding what this book is all about',\n 'and to the incomparable heide lange tireless champion of the da vinci code agent extraordinaire and trusted friend',\n 'i cannot fully express my gratitude to the exceptional team at doubleday for their generosity faith and superb guidance',\n 'thank you especially to bill thomas and steve rubin who believed in this book from the start',\n 'my thanks also to the initial core of early in house supporters headed by michael palgon suzanne herz janelle moburg jackie everly and adrienne sparks as well as to the talented people of doubleday s sales force']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cleaned_seq)","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"12293"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_seq2=list()\nfor line in cleaned_seq:\n    if line!='NA':\n        cleaned_seq2.append(line)\n\n","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cleaned_seq2)","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"11241"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"cleaned_seq2[1:10]","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"['more than ever',\n 'acknowledgments first and foremost to my friend and editor jason kaufman for working so hard on this project and for truly understanding what this book is all about',\n 'and to the incomparable heide lange tireless champion of the da vinci code agent extraordinaire and trusted friend',\n 'i cannot fully express my gratitude to the exceptional team at doubleday for their generosity faith and superb guidance',\n 'thank you especially to bill thomas and steve rubin who believed in this book from the start',\n 'my thanks also to the initial core of early in house supporters headed by michael palgon suzanne herz janelle moburg jackie everly and adrienne sparks as well as to the talented people of doubleday s sales force',\n 'for their generous assistance in the research of the book i would like to acknowledge the louvre museum the french ministry of culture project gutenberg bibliotheque nationale the gnostic society library the department of paintings study and documentation service at the louvre catholic world news royal observatory greenwich london record society the muniment collection at westminster abbey john pike and the federation of american scientists and the five members of opus dei three active two former who recounted their stories both positive and negative regarding their experiences inside opus dei',\n 'my gratitude also to water street bookstore for tracking down so many of my research books my father richard brown mathematics teacher and author for his assistance with the divine proportion and the fibonacci sequence stan planton sylvie baudeloque peter mcguigan francis mclnerney margie wachtel andre vernet ken kelleher at anchorball web media cara sottak karyn popham esther sung miriam abramowitz william tunstall pedoe and griffin wooden brown',\n 'and finally in a novel drawing so heavily on the sacred feminine i would be remiss if i did not mention the two extraordinary women who have touched my life']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"token=Tokenizer()","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token.fit_on_texts(cleaned_seq2)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#token.word_index","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# determine the vocabulary size\nvocab_size = len(token.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)","execution_count":21,"outputs":[{"output_type":"stream","text":"Vocabulary Size: 11186\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded=token.texts_to_sequences(cleaned_seq2)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoded[1]","execution_count":23,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"[76, 99, 305]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#encoded","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(encoded)","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"11241"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# We will create line based sequences "},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_list=list()","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in encoded:\n    for i in range(1,len(line)):\n        sequence_list.append(line[:i+1])\n        \n        ","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Total Sequences_list:',len(sequence_list))","execution_count":28,"outputs":[{"output_type":"stream","text":"Total Sequences_list: 131120\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_list[1:10]","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"[[1, 132, 135],\n [1, 132, 135, 286],\n [1, 132, 135, 286, 4673],\n [1, 132, 135, 286, 4673, 2332],\n [1, 132, 135, 286, 4673, 2332, 24],\n [1, 132, 135, 286, 4673, 2332, 24, 4674],\n [76, 99],\n [76, 99, 305],\n [6387, 157]]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Next , we need to pad the sequence using pad_sequence() in keras. Before that we need to find out the maximum lengh of the sequence so that we can pad all the sequences in the same length\n    \n# Pad Sequence"},{"metadata":{"trusted":true},"cell_type":"code","source":"max_length=max([len(seq) for seq in sequence_list])\nprint ('maximum sequence length is', max_length)","execution_count":30,"outputs":[{"output_type":"stream","text":"maximum sequence length is 90\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_padded_list=pad_sequences(sequence_list,maxlen=max_length,padding='pre')","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_padded_list[1:10]","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n         132,  135],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  132,\n         135,  286],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    1,  132,  135,\n         286, 4673],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    1,  132,  135,  286,\n        4673, 2332],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    1,  132,  135,  286, 4673,\n        2332,   24],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    1,  132,  135,  286, 4673, 2332,\n          24, 4674],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          76,   99],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   76,\n          99,  305],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n        6387,  157]], dtype=int32)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Next, we need to split the data to input and output elements\n\n# Split the data to input and output"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nsequence_padded_list=np.array(sequence_padded_list)\n\n","execution_count":34,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sequence_padded_list.shape","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"(131120, 90)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Split the input and output data\nX,y= sequence_padded_list[:,:-1],sequence_padded_list[:,-1]","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X[1:10]","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    1,\n         132],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    1,  132,\n         135],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    1,  132,  135,\n         286],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    1,  132,  135,  286,\n        4673],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    1,  132,  135,  286, 4673,\n        2332],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    1,  132,  135,  286, 4673, 2332,\n          24],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n          76],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,   76,\n          99],\n       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n        6387]], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X.shape","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"(131120, 89)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y[1:10]","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"array([ 135,  286, 4673, 2332,   24, 4674,   99,  305,  157], dtype=int32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.shape","execution_count":40,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"(131120,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical\n","execution_count":41,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vocab_size","execution_count":42,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"11186"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat=to_categorical(y,num_classes=vocab_size)","execution_count":43,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat[1:10]","execution_count":44,"outputs":[{"output_type":"execute_result","execution_count":44,"data":{"text/plain":"array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_cat.shape","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"(131120, 11186)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now , we need to train the model with the 131120 training examples each with max_length length.er \n\nFirst, we will create an embedding layer of 3 dimension as usual. \n\nvocab_size,\n\n10 dimesnion vector for each word, \n\n(max_length-1) sequence length. -1 , becuase we excluded the output y already\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":46,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout,Embedding,LSTM\nfrom keras.optimizers import RMSprop","execution_count":47,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We are going to use stacked LSTM with an additional LSTM layer "},{"metadata":{"trusted":true},"cell_type":"code","source":"# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\nmodel=Sequential()\nmodel.add(Embedding(vocab_size,40,input_length=max_length-1))\nmodel.add(LSTM(300,return_sequences=True))\nmodel.add(LSTM(200))\nmodel.add(Dense(vocab_size,activation='softmax'))\noptim=RMSprop(lr=0.07)\nmodel.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":49,"outputs":[{"output_type":"stream","text":"Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 89, 40)            447440    \n_________________________________________________________________\nlstm (LSTM)                  (None, 89, 300)           409200    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 200)               400800    \n_________________________________________________________________\ndense (Dense)                (None, 11186)             2248386   \n=================================================================\nTotal params: 3,505,826\nTrainable params: 3,505,826\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# Compile and fit the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X,y_cat,epochs=150,batch_size=50,verbose=2)","execution_count":50,"outputs":[{"output_type":"stream","text":"Epoch 1/150\n2623/2623 - 65s - loss: 6.9346 - accuracy: 0.0779\nEpoch 2/150\n2623/2623 - 65s - loss: 6.2754 - accuracy: 0.1018\nEpoch 3/150\n2623/2623 - 65s - loss: 5.8804 - accuracy: 0.1217\nEpoch 4/150\n2623/2623 - 65s - loss: 5.5962 - accuracy: 0.1338\nEpoch 5/150\n2623/2623 - 65s - loss: 5.3559 - accuracy: 0.1437\nEpoch 6/150\n2623/2623 - 65s - loss: 5.1307 - accuracy: 0.1536\nEpoch 7/150\n2623/2623 - 65s - loss: 4.9116 - accuracy: 0.1620\nEpoch 8/150\n2623/2623 - 64s - loss: 4.6975 - accuracy: 0.1709\nEpoch 9/150\n2623/2623 - 65s - loss: 4.4971 - accuracy: 0.1797\nEpoch 10/150\n2623/2623 - 65s - loss: 4.3076 - accuracy: 0.1921\nEpoch 11/150\n2623/2623 - 65s - loss: 4.1315 - accuracy: 0.2072\nEpoch 12/150\n2623/2623 - 65s - loss: 3.9713 - accuracy: 0.2231\nEpoch 13/150\n2623/2623 - 65s - loss: 3.8264 - accuracy: 0.2397\nEpoch 14/150\n2623/2623 - 64s - loss: 3.6926 - accuracy: 0.2560\nEpoch 15/150\n2623/2623 - 65s - loss: 3.5691 - accuracy: 0.2728\nEpoch 16/150\n2623/2623 - 65s - loss: 3.4559 - accuracy: 0.2880\nEpoch 17/150\n2623/2623 - 65s - loss: 3.3478 - accuracy: 0.3025\nEpoch 18/150\n2623/2623 - 64s - loss: 3.2475 - accuracy: 0.3178\nEpoch 19/150\n2623/2623 - 64s - loss: 3.1492 - accuracy: 0.3320\nEpoch 20/150\n2623/2623 - 64s - loss: 3.0560 - accuracy: 0.3474\nEpoch 21/150\n2623/2623 - 64s - loss: 2.9676 - accuracy: 0.3606\nEpoch 22/150\n2623/2623 - 65s - loss: 2.8811 - accuracy: 0.3765\nEpoch 23/150\n2623/2623 - 65s - loss: 2.7986 - accuracy: 0.3909\nEpoch 24/150\n2623/2623 - 65s - loss: 2.7191 - accuracy: 0.4040\nEpoch 25/150\n2623/2623 - 66s - loss: 2.6428 - accuracy: 0.4177\nEpoch 26/150\n2623/2623 - 65s - loss: 2.5685 - accuracy: 0.4305\nEpoch 27/150\n2623/2623 - 65s - loss: 2.4966 - accuracy: 0.4446\nEpoch 28/150\n2623/2623 - 65s - loss: 2.4269 - accuracy: 0.4563\nEpoch 29/150\n2623/2623 - 64s - loss: 2.3591 - accuracy: 0.4703\nEpoch 30/150\n2623/2623 - 65s - loss: 2.2927 - accuracy: 0.4837\nEpoch 31/150\n2623/2623 - 65s - loss: 2.2292 - accuracy: 0.4952\nEpoch 32/150\n2623/2623 - 65s - loss: 2.1671 - accuracy: 0.5071\nEpoch 33/150\n2623/2623 - 64s - loss: 2.1067 - accuracy: 0.5214\nEpoch 34/150\n2623/2623 - 65s - loss: 2.0471 - accuracy: 0.5323\nEpoch 35/150\n2623/2623 - 65s - loss: 1.9913 - accuracy: 0.5427\nEpoch 36/150\n2623/2623 - 65s - loss: 1.9369 - accuracy: 0.5553\nEpoch 37/150\n2623/2623 - 64s - loss: 1.8818 - accuracy: 0.5672\nEpoch 38/150\n2623/2623 - 64s - loss: 1.8300 - accuracy: 0.5781\nEpoch 39/150\n2623/2623 - 65s - loss: 1.7806 - accuracy: 0.5896\nEpoch 40/150\n2623/2623 - 64s - loss: 1.7322 - accuracy: 0.5996\nEpoch 41/150\n2623/2623 - 65s - loss: 1.6841 - accuracy: 0.6102\nEpoch 42/150\n2623/2623 - 64s - loss: 1.6385 - accuracy: 0.6213\nEpoch 43/150\n2623/2623 - 65s - loss: 1.5955 - accuracy: 0.6299\nEpoch 44/150\n2623/2623 - 65s - loss: 1.5540 - accuracy: 0.6394\nEpoch 45/150\n2623/2623 - 64s - loss: 1.5123 - accuracy: 0.6497\nEpoch 46/150\n2623/2623 - 64s - loss: 1.4739 - accuracy: 0.6576\nEpoch 47/150\n2623/2623 - 65s - loss: 1.4353 - accuracy: 0.6674\nEpoch 48/150\n2623/2623 - 65s - loss: 1.3987 - accuracy: 0.6758\nEpoch 49/150\n2623/2623 - 65s - loss: 1.3629 - accuracy: 0.6847\nEpoch 50/150\n2623/2623 - 65s - loss: 1.3281 - accuracy: 0.6926\nEpoch 51/150\n2623/2623 - 65s - loss: 1.2960 - accuracy: 0.6993\nEpoch 52/150\n2623/2623 - 65s - loss: 1.2642 - accuracy: 0.7076\nEpoch 53/150\n2623/2623 - 65s - loss: 1.2338 - accuracy: 0.7149\nEpoch 54/150\n2623/2623 - 65s - loss: 1.2033 - accuracy: 0.7210\nEpoch 55/150\n2623/2623 - 64s - loss: 1.1782 - accuracy: 0.7268\nEpoch 56/150\n2623/2623 - 64s - loss: 1.1508 - accuracy: 0.7337\nEpoch 57/150\n2623/2623 - 65s - loss: 1.1247 - accuracy: 0.7399\nEpoch 58/150\n2623/2623 - 65s - loss: 1.0986 - accuracy: 0.7457\nEpoch 59/150\n2623/2623 - 65s - loss: 1.0768 - accuracy: 0.7508\nEpoch 60/150\n2623/2623 - 66s - loss: 1.0536 - accuracy: 0.7559\nEpoch 61/150\n2623/2623 - 66s - loss: 1.0336 - accuracy: 0.7606\nEpoch 62/150\n2623/2623 - 66s - loss: 1.0121 - accuracy: 0.7654\nEpoch 63/150\n2623/2623 - 67s - loss: 0.9910 - accuracy: 0.7709\nEpoch 64/150\n2623/2623 - 66s - loss: 0.9721 - accuracy: 0.7753\nEpoch 65/150\n2623/2623 - 66s - loss: 0.9542 - accuracy: 0.7799\nEpoch 66/150\n2623/2623 - 66s - loss: 0.9378 - accuracy: 0.7837\nEpoch 67/150\n2623/2623 - 66s - loss: 0.9186 - accuracy: 0.7876\nEpoch 68/150\n2623/2623 - 66s - loss: 0.9055 - accuracy: 0.7897\nEpoch 69/150\n2623/2623 - 66s - loss: 0.8883 - accuracy: 0.7951\nEpoch 70/150\n2623/2623 - 67s - loss: 0.8764 - accuracy: 0.7973\nEpoch 71/150\n2623/2623 - 67s - loss: 0.8606 - accuracy: 0.8012\nEpoch 72/150\n2623/2623 - 66s - loss: 0.8490 - accuracy: 0.8040\nEpoch 73/150\n2623/2623 - 66s - loss: 0.8331 - accuracy: 0.8065\nEpoch 74/150\n2623/2623 - 66s - loss: 0.8252 - accuracy: 0.8090\nEpoch 75/150\n2623/2623 - 66s - loss: 0.8107 - accuracy: 0.8125\nEpoch 76/150\n2623/2623 - 67s - loss: 0.7993 - accuracy: 0.8151\nEpoch 77/150\n2623/2623 - 67s - loss: 0.7900 - accuracy: 0.8168\nEpoch 78/150\n2623/2623 - 67s - loss: 0.7763 - accuracy: 0.8200\nEpoch 79/150\n2623/2623 - 67s - loss: 0.7695 - accuracy: 0.8214\nEpoch 80/150\n2623/2623 - 67s - loss: 0.7592 - accuracy: 0.8234\nEpoch 81/150\n2623/2623 - 67s - loss: 0.7535 - accuracy: 0.8248\nEpoch 82/150\n2623/2623 - 67s - loss: 0.7410 - accuracy: 0.8274\nEpoch 83/150\n2623/2623 - 67s - loss: 0.7353 - accuracy: 0.8285\nEpoch 84/150\n2623/2623 - 67s - loss: 0.7280 - accuracy: 0.8301\nEpoch 85/150\n2623/2623 - 67s - loss: 0.7202 - accuracy: 0.8315\nEpoch 86/150\n2623/2623 - 67s - loss: 0.7119 - accuracy: 0.8335\nEpoch 87/150\n2623/2623 - 67s - loss: 0.7074 - accuracy: 0.8340\nEpoch 88/150\n2623/2623 - 67s - loss: 0.6982 - accuracy: 0.8356\nEpoch 89/150\n2623/2623 - 67s - loss: 0.6931 - accuracy: 0.8376\nEpoch 90/150\n2623/2623 - 66s - loss: 0.6900 - accuracy: 0.8381\nEpoch 91/150\n2623/2623 - 66s - loss: 0.6813 - accuracy: 0.8397\nEpoch 92/150\n2623/2623 - 65s - loss: 0.6801 - accuracy: 0.8394\nEpoch 93/150\n2623/2623 - 65s - loss: 0.6698 - accuracy: 0.8421\nEpoch 94/150\n2623/2623 - 65s - loss: 0.6632 - accuracy: 0.8436\nEpoch 95/150\n2623/2623 - 65s - loss: 0.6641 - accuracy: 0.8433\nEpoch 96/150\n2623/2623 - 66s - loss: 0.6602 - accuracy: 0.8435\nEpoch 97/150\n2623/2623 - 65s - loss: 0.6513 - accuracy: 0.8459\nEpoch 98/150\n2623/2623 - 65s - loss: 0.6482 - accuracy: 0.8459\nEpoch 99/150\n2623/2623 - 65s - loss: 0.6440 - accuracy: 0.8467\nEpoch 100/150\n2623/2623 - 65s - loss: 0.6406 - accuracy: 0.8473\nEpoch 101/150\n2623/2623 - 65s - loss: 0.6358 - accuracy: 0.8478\nEpoch 102/150\n2623/2623 - 64s - loss: 0.6374 - accuracy: 0.8475\nEpoch 103/150\n2623/2623 - 65s - loss: 0.6310 - accuracy: 0.8483\nEpoch 104/150\n2623/2623 - 64s - loss: 0.6280 - accuracy: 0.8495\nEpoch 105/150\n2623/2623 - 65s - loss: 0.6237 - accuracy: 0.8500\nEpoch 106/150\n2623/2623 - 65s - loss: 0.6199 - accuracy: 0.8505\nEpoch 107/150\n2623/2623 - 65s - loss: 0.6175 - accuracy: 0.8511\nEpoch 108/150\n2623/2623 - 65s - loss: 0.6164 - accuracy: 0.8513\nEpoch 109/150\n2623/2623 - 65s - loss: 0.6137 - accuracy: 0.8516\nEpoch 110/150\n2623/2623 - 65s - loss: 0.6127 - accuracy: 0.8513\nEpoch 111/150\n2623/2623 - 65s - loss: 0.6087 - accuracy: 0.8529\nEpoch 112/150\n2623/2623 - 64s - loss: 0.6032 - accuracy: 0.8534\nEpoch 113/150\n2623/2623 - 65s - loss: 0.6030 - accuracy: 0.8537\nEpoch 114/150\n2623/2623 - 65s - loss: 0.6048 - accuracy: 0.8518\nEpoch 115/150\n2623/2623 - 65s - loss: 0.5973 - accuracy: 0.8550\nEpoch 116/150\n2623/2623 - 64s - loss: 0.6023 - accuracy: 0.8528\nEpoch 117/150\n2623/2623 - 65s - loss: 0.5959 - accuracy: 0.8537\nEpoch 118/150\n2623/2623 - 64s - loss: 0.5912 - accuracy: 0.8548\nEpoch 119/150\n2623/2623 - 64s - loss: 0.5925 - accuracy: 0.8541\nEpoch 120/150\n2623/2623 - 65s - loss: 0.5907 - accuracy: 0.8553\nEpoch 121/150\n2623/2623 - 64s - loss: 0.5926 - accuracy: 0.8540\nEpoch 122/150\n2623/2623 - 65s - loss: 0.5904 - accuracy: 0.8551\nEpoch 123/150\n2623/2623 - 65s - loss: 0.5844 - accuracy: 0.8561\nEpoch 124/150\n2623/2623 - 65s - loss: 0.5845 - accuracy: 0.8560\nEpoch 125/150\n2623/2623 - 65s - loss: 0.5879 - accuracy: 0.8548\nEpoch 126/150\n2623/2623 - 64s - loss: 0.5778 - accuracy: 0.8577\nEpoch 127/150\n2623/2623 - 65s - loss: 0.5794 - accuracy: 0.8567\nEpoch 128/150\n2623/2623 - 64s - loss: 0.5809 - accuracy: 0.8557\nEpoch 129/150\n2623/2623 - 65s - loss: 0.5795 - accuracy: 0.8549\nEpoch 130/150\n2623/2623 - 64s - loss: 0.5823 - accuracy: 0.8547\n","name":"stdout"},{"output_type":"stream","text":"Epoch 131/150\n2623/2623 - 65s - loss: 0.5729 - accuracy: 0.8570\nEpoch 132/150\n2623/2623 - 64s - loss: 0.5770 - accuracy: 0.8559\nEpoch 133/150\n2623/2623 - 64s - loss: 0.5668 - accuracy: 0.8590\nEpoch 134/150\n2623/2623 - 65s - loss: 0.5728 - accuracy: 0.8566\nEpoch 135/150\n2623/2623 - 65s - loss: 0.5707 - accuracy: 0.8573\nEpoch 136/150\n2623/2623 - 64s - loss: 0.5745 - accuracy: 0.8561\nEpoch 137/150\n2623/2623 - 65s - loss: 0.5662 - accuracy: 0.8587\nEpoch 138/150\n2623/2623 - 64s - loss: 0.5712 - accuracy: 0.8570\nEpoch 139/150\n2623/2623 - 65s - loss: 0.5665 - accuracy: 0.8580\nEpoch 140/150\n2623/2623 - 64s - loss: 0.5691 - accuracy: 0.8570\nEpoch 141/150\n2623/2623 - 64s - loss: 0.5671 - accuracy: 0.8578\nEpoch 142/150\n2623/2623 - 64s - loss: 0.5700 - accuracy: 0.8561\nEpoch 143/150\n2623/2623 - 64s - loss: 0.5570 - accuracy: 0.8601\nEpoch 144/150\n2623/2623 - 65s - loss: 0.5652 - accuracy: 0.8575\nEpoch 145/150\n2623/2623 - 64s - loss: 0.5680 - accuracy: 0.8557\nEpoch 146/150\n2623/2623 - 64s - loss: 0.5670 - accuracy: 0.8560\nEpoch 147/150\n2623/2623 - 65s - loss: 0.5575 - accuracy: 0.8593\nEpoch 148/150\n2623/2623 - 64s - loss: 0.5603 - accuracy: 0.8583\nEpoch 149/150\n2623/2623 - 65s - loss: 0.5579 - accuracy: 0.8592\nEpoch 150/150\n2623/2623 - 64s - loss: 0.5567 - accuracy: 0.8594\n","name":"stdout"},{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f8373ee5e10>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Note : As we observe in the above execution , accuracy is not much improved after 100 iterations.\nSo its always a best practice **'Early Stopping'**. This will avoid overfitting"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('keras_next_word_model_StakckedLSTM.h5')","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":52,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model1=keras.models.load_model('./keras_next_word_model_StakckedLSTM.h5')\n\n","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now, lets test the model. we will generate a sequence using the below methods"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Generate sequence from language model\n\ndef generate_seq(model,token,max_length,seed_text,n_words):\n    in_text=seed_text\n    \n    # generate the fixed number of words given by n_words in the input to the function\n    for _ in range(n_words):\n        # encode the text as integeer\n        encoded_text=token.texts_to_sequences([in_text])[0]\n        #prepad same as we did before training\n        encoded_text=pad_sequences([encoded_text],maxlen=max_length,padding='pre')\n        #predict the probabilities of the word\n        y_pred=model.predict_classes(encoded_text,verbose=0)\n        \n        #map the index to the word\n        \n        outword=''\n        for word, index in token.word_index.items():\n            if index == y_pred:\n                out_word = word\n                break\n        # append to input\n        in_text += ' ' + out_word\n    \n    return in_text","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(generate_seq(model1,token,max_length-1,'Langdon',10))","execution_count":56,"outputs":[{"output_type":"stream","text":"Langdon had no idea how to respond about rosslyn s tomb\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(generate_seq(model1,token,max_length-1,'Professor',10))","execution_count":57,"outputs":[{"output_type":"stream","text":"hostess is supposed to be with respect that very sorry for\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}