{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Read the data\n\ndata can be dowloaded from http://www.manythings.org/anki/"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import zipfile\nimport io","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=''\nwith open('../input/englishtohindi/hin.txt','rb') as readfile:\n    for line in io.TextIOWrapper(readfile,'utf-8'):\n        data+=line","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data=data.split('\\n')\ndata[1:20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data[0].split('\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Review the data"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Seperate the encoder and decoder input"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_text=[]\ndecoder_text=[]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for line in data:\n    try:\n        in_text,out_txt,_=line.split('\\t')\n        encoder_text.append(in_text)\n        decoder_text.append('<start>'+out_txt+'<end>')\n    except:\n        pass\n            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_text[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_text[1:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build input and output sequence"},{"metadata":{"trusted":true},"cell_type":"code","source":"import keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_t=Tokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_t.fit_on_texts(encoder_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_seq=encoder_t.texts_to_sequences(encoder_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_seq[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_max_lenth=max([len(txt) for txt in encoder_seq])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_max_lenth","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_vocab_size=len(encoder_t.word_index)\nencoder_vocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Same steps for decoder also**"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_t=Tokenizer()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_t.fit_on_texts(decoder_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#decoder_t.word_index.ix[1:10]\nimport itertools\n\n#(itertools.islice( (decoder_t.word_index()),10)\n#take(10, decoder_t.word_index())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(decoder_t.word_index.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_seq=decoder_t.texts_to_sequences(decoder_text)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_seq[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_max_length=max([len(text) for text in decoder_seq])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_max_length","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_vocab_size=len(decoder_t.word_index)\ndecoder_vocab_size","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Padding both the sequences"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data=pad_sequences(encoder_seq,maxlen=encoder_max_lenth,padding='post')\ndecoder_input_data=pad_sequences(decoder_seq,maxlen=decoder_max_length,padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_input_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data[1:10]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_input_data[1:10]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building Decoder output"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_data=np.zeros((decoder_input_data.shape[0],decoder_input_data.shape[1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#shifting the data by one unit for each row , since it is a prediction of the next word\nfor i in range(decoder_input_data.shape[0]):\n    for j in range(1,decoder_input_data.shape[1]):\n        decoder_target_data[i][j-1]=decoder_input_data[i][j]\n        \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max(map(max, decoder_target_data))\n\n# Maximum value of decoder target data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_data[:26]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_input_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_input_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_data[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"(decoder_vocab_size)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**convert this data to one hot encoding for softmax**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils import to_categorical","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_onehot=np.zeros((decoder_target_data.shape[0],decoder_target_data.shape[1],decoder_vocab_size+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_t.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(decoder_target_data.shape[0]):\n    for j in range(decoder_target_data.shape[1]):\n        decoder_target_onehot[i][j]=to_categorical(decoder_target_data[i][j],num_classes=decoder_vocab_size+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_vocab_size-1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_onehot.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_target_onehot[0]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the training Model\n**We are going to use the Keras functional API this time**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.layers import Input,Embedding,Dense,LSTM","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define the embeding vector length\n\nencoder_embeding_size=50\ndecoder_embeding_size=50","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Build the encoder**"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input=Input(shape=(encoder_max_lenth,))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_embedding=Embedding(encoder_vocab_size+1,encoder_embeding_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_embedding_output=encoder_embedding(encoder_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x,state_h,state_c=LSTM(256,return_state=True)(encoder_embedding_output)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**return_state =True** actually means:\n\nthe output of an LSTM is the **hidden state (h)** which is x in the above case. Now, we need the **cell state (c)** of LSTM to pass\nit to the decoder. so we use return_state=True attribute\n\nHere x and state_h are one and the same which are the hidden state of the LSTM at last time step"},{"metadata":{"trusted":true},"cell_type":"code","source":"state_h.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"state_c.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_states = [state_h, state_c]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Build a decoder"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_input=Input(shape=(None,)) \n#( This means None * None,any number of inputs and each input any number of words)\n# This is as good as Input(shape=decoder_max_lenth,)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_embedding=Embedding(decoder_vocab_size+1,decoder_embeding_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_embedding_output=decoder_embedding(decoder_input)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_rnn=LSTM(256,return_sequences=True,return_state=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After the embeding layer , now we are supplying 2 inputs to LSTM layer.\n\n1. Previous states ( which would have been set by model to some default values similart to encoder, if we do not pass it to decoder)\n\n2. current embedding output which is the target language in the vector form"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Initialize initial state with encoder_states\n#Output will be all hidden sequences, last 'h' state and last 'c' state\n\nx,_,_=decoder_rnn(decoder_embedding_output,initial_state=encoder_states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_dense=Dense(decoder_vocab_size+1,activation='softmax')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_outputs=decoder_dense(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_outputs.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Now ,the final step\n\n# we have encoder input,decoder input, decoder output and the encoder and decoder layers"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here we are sending the decoder inputs during training. This is important point to be noted. \nIt is also called Techer Forcing technique. This is to make sure that when the first prediction of the target goes wrong, we dont want to send the predicted output to the next timestep. If we send the predicted targetword to the next step, it is most likely that the entire target sequence will be wrong.\nTo avoid this, we send the target sequence also in the training**"},{"metadata":{"trusted":true},"cell_type":"code","source":"model=Model([encoder_input,decoder_input],[decoder_outputs])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Train the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([encoder_input_data, decoder_input_data], decoder_target_onehot,\n          batch_size=64,\n          epochs=150)\n          #validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('seq2seq_eng_hin_training.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Building the model for prediction "},{"metadata":{},"cell_type":"markdown","source":"## Build the encoder model"},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_model=Model(encoder_input,encoder_states)\n\n# This means during prediction we are going to send the encoder inputs, i.e. engish text and we get the \n# states ( Cell and hidden) from the encoder","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Build the decoder model\n\n   1. Define both the states h & C initialization\n   2. Get the output from Encoder\n   3. Define the decoder output\n   4. Build the model\n   \n   \n\n\n\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"rnn_units=256 ## LSTM states","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_state_input_h = Input(shape=(rnn_units,)) \n# This means we are going to have 256 hidden states for each sentense becuase the rerutn_state=True for encoder \n# When we say return_state=True, it gives us the state of the LSTM for the last time step. Since we have \n#256 LSTM cells, each cell return one hidden state . \n# Hence for each sequence we will get 256 and for a batch size None*256","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_state_input_c = Input(shape=(rnn_units,))\n#Same comment as above for the hidden states. Last time step for each sequence we will get the cell state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n# This is a list of both that is going to be the decoder input","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = decoder_embedding(decoder_input) \n# We are going to use the same embedding layer of the decoder that we have trained.\n# But in this case we are not going to send the input to decoder since we dont have any during prediction\n# We are just going to send the 'START' word to start predicting the corresponding next work that it understood\n# from training for a given combination of cell state and hidden state","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use the layer which we trained earlier i.e. decoder_rnn\nrnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This time we are retaining the decoder states(hidden/cell) for each word\n#Why do we need this?\n#We need this to predict the sequence of target language words.i.e next word\ndecoder_states = [state_h, state_c]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last layer is Dense layer . Again we are going to use the same that we have trained before decoder_dense\n\ndecoder_outputs = decoder_dense(rnn_outputs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Finally, building the decoder model"},{"metadata":{"trusted":true},"cell_type":"code","source":"decoder_model = Model([decoder_input] + decoder_states_inputs,  #Model inputs i.e. 'START' for the first time and the predicted word next time onwards\n                     [decoder_outputs] + decoder_states)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predicting Output"},{"metadata":{"trusted":true},"cell_type":"code","source":"# before starting the prediction we will just have the dictionary of targetword and the index\nint_to_word_decoder = dict((i,c) for c, i in decoder_t.word_index.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(int_to_word_decoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"int_to_word_decoder","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sentence(input_sequence):\n    #Get the encoder state values\n    decoder_initial_states_value = encoder_model.predict(input_sequence)\n    \n    #Build a sequence with '<start>' - starting sequence for Decoder. As mentioned earlier, we are going \n    # to send only '<Start>' to start predicting the next work for decoder \n    target_seq = np.zeros((1,1))    \n    target_seq[0][0] = decoder_t.word_index['start']\n    \n    \n    #flag to check if prediction should be stopped\n    stop_loop = False\n    \n    \n    \n    #Initialize predicted sentence\n    predicted_sentence = ''\n   \n    #start the loop\n    while not stop_loop:\n        \n        # We are passsing the initial states as encoder output & 'Start' for the first time.\n        # At the end of the loop these values will be updated both target_seq & \n        predicted_outputs, h, c = decoder_model.predict([target_seq] + \n                                                        decoder_initial_states_value)\n        \n       # print('shape of the predicted output',predicted_outputs.shape)\n       # print(predicted_outputs)\n          #Get the predicted output with highest probability\n       # print( 'predicted output',predicted_outputs[0,-1])\n        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n        \n        #print(predicted_output)\n        \n         #Get the predicted word from predicter integer\n        predicted_word = int_to_word_decoder[predicted_output]\n        \n        \n        #Check if prediction should stop\n        if(predicted_word == 'end' or len(predicted_sentence) > 27):# max_decoder_seq_length=27\n            \n            stop_loop = True\n            continue\n            \n        \n        \n         #Updated predicted sentence. First time replace with the prediction of start\n        if (len(predicted_sentence) == 0):\n            predicted_sentence = predicted_word\n        else:  \n            # second time onwards, concatenate the current prediction with the previous prediction\n            \n            predicted_sentence = predicted_sentence + ' ' + predicted_word\n            \n        \n          #Update target_seq to be the predicted word index\n        target_seq[0][0] = predicted_output\n        \n          \n            \n            #Update initial states value for decoder. For the first time encoder output is used\n            #next time onwards, its the decoder states\n        decoder_initial_states_value = [h,c]  \n    \n    \n    \n    return predicted_sentence\n            \n            \n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# We will call the above prediction with some random text to predict the target sequence"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get a random sentence\nstart_num = np.random.randint(0, high=len(encoder_text) - 10)\n#print(start_num)\n\nfor i in range(start_num, start_num + 10):\n    input_seq = encoder_input_data[i : i+1]\n   # print(input_seq)\n    predicted_sentence = decode_sentence(input_seq)\n    print('--------')\n    print ('Input sentence: ', encoder_text[i])\n    print ('Predicted sentence: ', predicted_sentence )","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}